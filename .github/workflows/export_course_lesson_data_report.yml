name: Export course lesson data report to BigQuery

on: workflow_dispatch

jobs:
  deploy:
    runs-on: ubuntu-20.04
    container: google/cloud-sdk:latest
    steps:
      - name: Auth with gcloud
        uses: google-github-actions/setup-gcloud@master
        with:
          service_account_key: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_BASE64 }}

      - name: Install CloudFoundry CLI
        shell: bash
        run: |
          apt-get install -y wget postgresql-client
          wget -q -O - https://packages.cloudfoundry.org/debian/cli.cloudfoundry.org.key | apt-key add -
          echo "deb https://packages.cloudfoundry.org/debian stable main" | tee /etc/apt/sources.list.d/cloudfoundry-cli.list
          apt-get update
          apt-get install cf7-cli
      - name: Install conduit plugin
        shell: bash
        run: |
          cf install-plugin conduit -f

      - name: Generate dashboard report
        shell: bash
        env:
          PAAS_ORGANISATION: dfe
          PAAS_SPACE: early-careers-framework-staging
          ENV_STUB: staging
          APP_NAME: ecf-engage-and-learn
          CF_USERNAME: ${{ secrets.GOVPAAS_STAG_USERNAME }}
          CF_PASSWORD: ${{ secrets.GOVPAAS_STAG_PASSWORD }}
        run: |
          cf api https://api.london.cloud.service.gov.uk
          cf auth
          cf target -o "${{ env.PAAS_ORGANISATION }}" -s "${{ env.PAAS_SPACE }}"
          cf run-task "${{ env.APP_NAME }}"-"${{ env.ENV_STUB }}" --command "cd /app && /usr/local/bundle/bin/bundle exec rails runner 'CourseLessonDataReportJob.perform_now'"

      - name: Download csv
        shell: bash
        env:
          CF_USERNAME: ${{ secrets.GOVPAAS_STAG_USERNAME }}
          CF_PASSWORD: ${{ secrets.GOVPAAS_STAG_PASSWORD }}
          PAAS_ORGANISATION: dfe
          PAAS_SPACE: early-careers-framework-staging
        run: |
          cf api https://api.london.cloud.service.gov.uk
          cf auth
          cf target -o "${{ env.PAAS_ORGANISATION }}" -s "${{ env.PAAS_SPACE }}"
          cf conduit ecf-engage-and-learn-staging-db -c '{"read_only": true}' -- psql -d rdsbroker_79c0fbce_1680_46f1_8ff0_5b176da3d6d1 -c "copy(select data from reports where identifier='course_lesson_data') to STDOUT" | sed 's/\\n/\n/g' > ~/data.csv

      - name: Upload csv to BigQuery
        shell: bash
        run: |
          bq load --autodetect --replace engage_and_learn.course_lesson_data ~/data.csv
